import pandas as pd
import json
import os
import time
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain.schema import Document
from dotenv import load_dotenv, find_dotenv
from tqdm import tqdm
from absl import logging
import vertexai
from vertexai.generative_models import HarmCategory, HarmBlockThreshold
from vertexai.preview.generative_models import GenerativeModel

logging.set_verbosity(logging.ERROR)

# Load environment variables
load_dotenv(find_dotenv())

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

if not all([GOOGLE_API_KEY, PINECONE_API_KEY]):
    exit("Missing required API keys.")

# Initialize Vertex AI
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "email-extraction-381718-3f73208ce3b71.json"
vertexai.init(project="email-extraction-381718", location="us-central1")

generative_multimodal_model = GenerativeModel("gemini-1.5-pro-001")

generation_config = {
    "max_output_tokens": 8192,
    "temperature": 0.4,
    "top_p": 0.5,
}

safety_settings = {
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
}

base_dir = "C:/Users/SuvarnaSampath/PycharmProjects/Superset"
base_folder = os.path.join(base_dir, "Base")
data_folder = os.path.join(base_dir, "Data")
os.makedirs(data_folder, exist_ok=True)

txt_files = [f for f in os.listdir(base_folder) if f.endswith(".txt")]

if len(txt_files) != 1:
    exit(f"Expected exactly 1 .txt file in {base_folder}, but found {len(txt_files)}")

txt_file_path = os.path.join(base_folder, txt_files[0])
csv_file_path = os.path.join(data_folder, os.path.splitext(txt_files[0])[0] + ".csv")


def json_to_dataframe(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            data = json.load(file)

        if isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
            df = pd.DataFrame(data["data"])
        else:
            raise ValueError("Invalid JSON format: Expected a list or a dictionary with a 'data' key.")

        return df

    except json.JSONDecodeError as e:
        exit(f"Error parsing JSON: {e}")
    except Exception as e:
        exit(f"Unexpected error: {e}")


df = json_to_dataframe(txt_file_path)
df.to_csv(csv_file_path, index=False)
print(f"Converted and saved CSV at: {csv_file_path}")


def group_rows_into_chunks(df, rows_per_chunk=40):
    text_chunks = []
    for i in range(0, len(df), rows_per_chunk):
        chunk_df = df.iloc[i:i + rows_per_chunk]
        chunk_text = "\n".join(chunk_df.astype(str).apply(lambda row: " | ".join(row), axis=1))
        text_chunks.append(Document(page_content=chunk_text))
    return text_chunks


# Load CSV
df = pd.read_csv(csv_file_path)
chunks = group_rows_into_chunks(df, rows_per_chunk=40)

embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "rag-index"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=768,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
    time.sleep(10)

try:
    index = pc.Index(index_name)
    index_stats = index.describe_index_stats()
except Exception as e:
    exit(f"Error: Failed to connect to Pinecone index - {e}")

if index_stats["total_vector_count"] == 0:
    vectors = []
    for i, doc in tqdm(enumerate(chunks), total=len(chunks), desc="Uploading Chunks"):
        embedding = embedding_model.embed_query(doc.page_content)
        vectors.append((f"doc_{i}", embedding, {"text": doc.page_content}))

    index.upsert(vectors=vectors)
    print("Embeddings successfully stored in Pinecone.")
else:
    print("Embeddings already exist in Pinecone. Skipping upload.")


def retrieve_and_generate_response(user_query, top_k=3):
    query_embedding = embedding_model.embed_query(user_query)
    search_results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)

    if not search_results["matches"]:
        return "No relevant information found. Try rephrasing the query."

    retrieved_texts = [match["metadata"]["text"] for match in search_results["matches"]]
    retrieved_text = "\n".join(retrieved_texts)

    def generate_response(retrieved_text, user_query):
        prompt = (
            "You are an AI assistant analyzing data. Below is the retrieved data relevant to the query. "
            "Answer the user's question based only on this data. Provide a structured, concise, and informative response.\n\n"
            f"Retrieved Data:\n{retrieved_text}\n\n"
            f"User Query: {user_query}")
        response = generative_multimodal_model.generate_content(prompt, generation_config=generation_config,
                                                                safety_settings=safety_settings)
        return response.text

    return generate_response(retrieved_text, user_query)


try:
    while True:
        user_input = input("Ask your query (or type 'exit' to quit): ").strip()
        if user_input.lower() in {"exit", "quit"}:
            break

        response = retrieve_and_generate_response(user_input)
        print("\nAI Response:", response, "\n")

finally:
    print("Cleaning up: Deleting Pinecone index...")
    pc.delete_index(index_name)
    print(f"Index '{index_name}' deleted successfully.")

    os.remove(txt_file_path)
    os.remove(csv_file_path)
    print(f"Deleted files:\n - {txt_file_path}\n - {csv_file_path}")
