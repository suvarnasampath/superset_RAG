import os
import json
import time
import pandas as pd
from dotenv import load_dotenv, find_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from langchain.schema import Document
from langchain_groq import ChatGroq
from tqdm import tqdm

load_dotenv(find_dotenv())

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "rag-index"

def convert_json_to_csv(file_path, output_folder):
    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    if isinstance(data, list):
        df = pd.DataFrame(data)
    elif isinstance(data, dict) and "data" in data and isinstance(data["data"], list):
        df = pd.DataFrame(data["data"])
    else:
        raise ValueError("Invalid JSON format")

    csv_path = os.path.join(output_folder, os.path.splitext(os.path.basename(file_path))[0] + ".csv")
    df.to_csv(csv_path, index=False)
    return csv_path

def group_rows_into_chunks(df, rows_per_chunk=40):
    chunks = []
    for i in range(0, len(df), rows_per_chunk):
        chunk_df = df.iloc[i:i + rows_per_chunk]
        chunk_text = "\n".join(chunk_df.astype(str).apply(lambda row: " | ".join(row), axis=1))
        chunks.append(Document(page_content=chunk_text))
    return chunks

def ensure_pinecone_index():
    if index_name not in pc.list_indexes().names():
        pc.create_index(
            name=index_name,
            dimension=768,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
        time.sleep(10)

def upload_embeddings_to_pinecone(chunks):
    ensure_pinecone_index()
    index = pc.Index(index_name)

    if index.describe_index_stats()["total_vector_count"] > 0:
        return

    vectors = []
    for i, doc in tqdm(enumerate(chunks), total=len(chunks), desc="Uploading"):
        embedding = embedding_model.embed_query(doc.page_content)
        vectors.append((f"doc_{i}", embedding, {"text": doc.page_content}))

    index.upsert(vectors=vectors)

def query_and_generate_response(user_query):
    ensure_pinecone_index()
    index = pc.Index(index_name)

    embedding = embedding_model.embed_query(user_query)
    search_results = index.query(vector=embedding, top_k=3, include_metadata=True)

    if not search_results["matches"]:
        return "No relevant information found. Try rephrasing the query."

    retrieved_texts = [match["metadata"]["text"] for match in search_results["matches"]]
    retrieved_text = "\n".join(retrieved_texts)

    chat_model = ChatGroq(model_name="llama-3.1-8b-instant", api_key=GROQ_API_KEY)
    response = chat_model.invoke(
        f"Here is inventory data retrieved for the query:\n\n{retrieved_text}\n\nAnswer the query based only on this data: {user_query}"
    )

    return response.content

def cleanup_files_and_index(file_path, csv_path):
    if os.path.exists(file_path):
        os.remove(file_path)
    if os.path.exists(csv_path):
        os.remove(csv_path)
    if index_name in pc.list_indexes().names():
        pc.delete_index(index_name)
